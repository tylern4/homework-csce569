\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{verbatimbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subfigure}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  numberstyle=\tiny,
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\title{CSCE 569: Homework 3}
\author{Nick Tyler}
\date{03/30/18}

\begin{document}

\maketitle

\section*{Jacobi Iterative Method}
There are three separate parts where the sequential version of the Jacobi method must be modified in order to make it parallel. The first thing that needs to be modified is to take the full matrix and decompose it by rows.

\begin{lstlisting}[language=C++]
  int rows_to_send;
  int rows_to_process = n / numprocs;
  if (myrank == 0 || myrank == numprocs - 1)
    rows_to_send = rows_to_process + 1;
  else
    rows_to_send = rows_to_process + 2;
  if (myrank == 0) {
    int send_to_rank;
    for (send_to_rank = 1; send_to_rank < numprocs; send_to_rank++) {
      int num_sending;
      if (send_to_rank == numprocs - 1)
        num_sending = rows_to_process + 1;
      else
        num_sending = rows_to_process + 2;
      uptr = umpi + (send_to_rank * rows_to_process - 1) * m;
      fptr = fmpi + (send_to_rank * rows_to_process - 1) * m;

      MPI_Isend(uptr, num_sending * m, MPI_FLOAT, send_to_rank, send_to_rank,
                MPI_COMM_WORLD, &request);
      MPI_Isend(fptr, num_sending * m, MPI_FLOAT, send_to_rank, send_to_rank,
                MPI_COMM_WORLD, &request);
    }
  } else {
    umpi = malloc(rows_to_send * m * sizeof(REAL));
    fmpi = malloc(rows_to_send * m * sizeof(REAL));
    MPI_Irecv(umpi, rows_to_send * m, MPI_FLOAT, 0, myrank, MPI_COMM_WORLD,
              &request);
    MPI_Irecv(fmpi, rows_to_send * m, MPI_FLOAT, 0, myrank, MPI_COMM_WORLD,
              &request);
  }
\end{lstlisting}

Once the matrix is splint and sent to all the processes the Jacobi method is called by all the processes on their respective matrix part. In the Jacobi method the boundary rows must be exchanged between processes and requires a second set of mpi calls. The last row of one section must exchanged with the first row of the adjacent section. To do this the first row, $uold[0]$, was exchanged with the last row $uold[(n-1)*m]$.

\begin{lstlisting}[language=C++]
    if (myrank == 0) {
      // send last row to myrank==1
      MPI_Send(uold[(n - 1) * m], m, MPI_FLOAT, myrank + 1, myrank + 1,
               MPI_COMM_WORLD);
      // recv last row from myrank==1
      MPI_Recv(uold[(n - 1) * m], m, MPI_FLOAT, myrank + 1, myrank,
               MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    } else if (myrank == numprocs - 1) {
      // send first row to numprocs-2
      MPI_Send(uold[0], m, MPI_FLOAT, numprocs - 2, numprocs - 2,
               MPI_COMM_WORLD);
      // recv first row from numprocs-2
      MPI_Recv(uold[0], m, MPI_FLOAT, numprocs - 2, myrank, MPI_COMM_WORLD,
               MPI_STATUS_IGNORE);
    } else {
      // send last row to myrank+1
      MPI_Send(uold[(n - 1) * m], m, MPI_FLOAT, myrank + 1, myrank + 1,
               MPI_COMM_WORLD);
      // recv last row from myrank+1
      MPI_Recv(uold[(n - 1) * m], m, MPI_FLOAT, myrank - 1, myrank,
               MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      // send first row to myrank-1
      MPI_Send(uold[0], m, MPI_FLOAT, myrank - 1, myrank - 1, MPI_COMM_WORLD);
      // recv first row from myrank-1
      MPI_Recv(uold[0], m, MPI_FLOAT, myrank - 1, myrank, MPI_COMM_WORLD,
               MPI_STATUS_IGNORE);
    }
\end{lstlisting}

This is where my program fails when running. It gives errors that the memory address is not set properly that I was not able to diagnose and solve.

After this the iterative method was performed by each process on it's respective section of the matrix. The error was computed for each section and then was added back together with and MPI\_Allreduce call.

\begin{lstlisting}[language=C++]
MPI_Allreduce(&error, &gerror, 1, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
\end{lstlisting}

After the iterations was completed the results were compiled back together, similar to the first step but in reverse.

\begin{lstlisting}[language=C++]
  if (myrank != 0) {
    int num_sending;
    if (myrank == numprocs - 1)
      num_sending = rows_to_process + 1;
    else
      num_sending = rows_to_process + 2;

    MPI_Isend(uptr, num_sending * m, MPI_FLOAT, 0, myrank, MPI_COMM_WORLD,
              &request);
  } else {
    int recv_rank;
    for (recv_rank = 1; recv_rank < numprocs; recv_rank++) {
      int rows_to_recv;
      if (recv_rank == numprocs - 1)
        rows_to_recv = rows_to_process + 1;
      else
        rows_to_recv = rows_to_process + 2;

      MPI_Irecv(umpi, rows_to_recv * m, MPI_FLOAT, recv_rank, 0, MPI_COMM_WORLD,
                &request);
    }
  }
\end{lstlisting}

With more time I would solve the issue with the memory addresses issue and be able to test on bridges to get timing information.

\pagebreak

\end{document}
              
            
